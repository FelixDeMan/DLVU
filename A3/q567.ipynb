{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import numpy as np\n",
    "from data import load_ndfa, load_brackets\n",
    "import logging\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "logging.basicConfig(level='DEBUG', format='%(asctime)s - %(message)s')\n",
    "RANDOM_SEED = 10\n",
    "\n",
    "MAX_CHARS_PER_BATCH = 10000\n",
    "DATASET = 'ndfa'\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "EVAL_N_SAMPLES = 10\n",
    "EVAL_SEQ_MAX_LEN = 20\n",
    "\n",
    "class HParams:\n",
    "    EMB_DIM = 32\n",
    "    N_HIDDEN = 16\n",
    "    N_LAYERS = 1\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "dataset_loaders = {\n",
    "    'ndfa' : load_ndfa,\n",
    "    'brackets': load_brackets\n",
    "}\n",
    "    \n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def prepare_batches(x_train, i2w, w2i, max_chars_per_batch):\n",
    "    dict_size = len(i2w)\n",
    "    \n",
    "    # w2i['.pad'] = 0\n",
    "    pad_val = w2i['.pad']\n",
    "    # w2i['.start'] = 1\n",
    "    start_val = w2i['.start']\n",
    "    # w2i['.end'] = 2\n",
    "    end_val = w2i['.end']\n",
    "\n",
    "    for x in x_train:\n",
    "        x.insert(0, start_val)\n",
    "        x.append(end_val)\n",
    "\n",
    "    sizes = defaultdict(list)\n",
    "    for x in x_train:\n",
    "        sizes[len(x)].append(x)\n",
    "\n",
    "    t_sizes = dict()\n",
    "    for k, v in sizes.items():\n",
    "        t_sizes[k] = torch.tensor(v, dtype=torch.long)\n",
    "    \n",
    "    batches = []\n",
    "    for _, x_tensor in t_sizes.items():\n",
    "        x_tensor_len, n_chars = x_tensor.shape\n",
    "        \n",
    "        # Shift input left to create output tensor\n",
    "        shifted_input = x_tensor[:, 2:]\n",
    "        \n",
    "        # Make column with padding value\n",
    "        start_pad = start_val * torch.ones(x_tensor_len, dtype=torch.long)\n",
    "\n",
    "        empty_pad = pad_val * torch.ones(x_tensor_len, dtype=torch.long)\n",
    "\n",
    "        # Append padding to output tensor\n",
    "        y_tensor = torch.column_stack([start_pad, shifted_input, empty_pad.T])\n",
    "\n",
    "        assert x_tensor.shape == y_tensor.shape\n",
    "\n",
    "        # Split into batches\n",
    "        batch_size = max_chars_per_batch // n_chars\n",
    "        x_batches = torch.split(x_tensor, batch_size)\n",
    "        y_batches = torch.split(y_tensor, batch_size)\n",
    "        \n",
    "        # Create One-Hot Encodings of the output\n",
    "        # TODO probably there is a smarter way to do one hots over the whole dict\n",
    "        y_oh_batches = list()\n",
    "        for y in y_batches:\n",
    "            b, chrs = y.shape\n",
    "            one_hots = torch.zeros(b,chrs, dict_size, dtype=torch.long)\n",
    "            for bi in range(y.shape[0]):\n",
    "                y_one_hot = torch.zeros(chrs, dict_size, dtype=torch.long)\n",
    "                for el in range(chrs):\n",
    "                    y_one_hot[el][y[bi,el]] = 1\n",
    "                one_hots[bi, :, :] = y_one_hot\n",
    "            y_oh_batches.append(one_hots)\n",
    "        \n",
    "        assert len(x_batches) == len(y_oh_batches)\n",
    "        batches.extend(list(zip(x_batches, y_oh_batches)))\n",
    "    \n",
    "    np.random.shuffle(batches)\n",
    "    return batches\n",
    "\n",
    "class recurNet(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim = 32, hidden_size = 16, vocab_size = 15, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.layer1 = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)\n",
    "\n",
    "        self.layer2 = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.layer3 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # b, n_chrs = input.shape\n",
    "        emb = self.layer1(input)\n",
    "        # assert emb.shape == (b, n_chrs, self.embedding_dim)\n",
    "                    \n",
    "        lstm, (_, _) = self.layer2(emb)\n",
    "\n",
    "        # assert lstm.shape == (b, n_chrs, self.hidden_size)\n",
    "        output = self.layer3(lstm)\n",
    "        # assert output.shape == (b, n_chrs, self.vocab_size)\n",
    "\n",
    "        return output\n",
    "        \n",
    "def print_some_batches(batches, i2w):\n",
    "    for x,y in batches:\n",
    "        i = random.choice(list(range(len(x))))\n",
    "        print(f'Input | {\" \".join(decode(x[i], i2w))}')\n",
    "        print(f'Output | {\" \".join(decode(y[i].argmax(1), i2w))}')\n",
    "\n",
    "def train(model: nn.Module, epochs : int, batches: List[Tuple[torch.Tensor, torch.Tensor]], device: torch.device, lr: float, loss_print_freq: int=30):\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    # Capture training starting time\n",
    "    ts_train = time.perf_counter()\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    # list for training progress capturing\n",
    "    data = list()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Capture epoch starting time\n",
    "        ts = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        for i, (x,one_hots) in enumerate(batches):\n",
    "            x, one_hots = x.to(device), one_hots.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(x)\n",
    "            \n",
    "            loss = criterion(out, one_hots.type(torch.float32))\n",
    "            \n",
    "            # divide by batch and # of tokens\n",
    "            loss /= one_hots.shape[0]* one_hots.shape[1]\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % loss_print_freq == 0: #print every 1000 batches\n",
    "                logging.info('[%d, %5d] loss: %.3f ' %\n",
    "                    (epoch +1, i+1, running_loss / loss_print_freq))\n",
    "                running_loss = 0.0\n",
    "            data.append({'update' : i, 'epoch': epoch, 'loss': loss.item()})\n",
    "        logging.info(f'Epoch took: {time.perf_counter()-ts:.2f}s')\n",
    "    \n",
    "    logging.info(f'Finished training. {epochs} epochs took: {time.perf_counter()-ts_train:.2f}s')\n",
    "    return data, model\n",
    "\n",
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome logits :param temperature: Sampling temperature. 1.0 follows the given\n",
    "    distribution, 0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element. \"\"\" \n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = F.softmax(lnprobs / temperature, dim=0) \n",
    "    cd = dist.Categorical(p)\n",
    "    return cd.sample()\n",
    "\n",
    "    \n",
    "def decode(seq, i2w):\n",
    "    return [i2w[tok] for tok in seq]\n",
    "\n",
    "def sample_model(model: nn.Module, seed_seq: List[int], end_token : int, device: torch.device, max_len:int=20, temperature: float =1.0):\n",
    "    seed_seq = deepcopy(seed_seq)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while len(seed_seq) <= max_len and seed_seq[-1] != end_token:\n",
    "            seed_tensor = torch.LongTensor(seed_seq).view(1,-1)\n",
    "            seed_tensor = seed_tensor.to(device)\n",
    "        \n",
    "            logits = model(seed_tensor)\n",
    "            \n",
    "            logits = logits.cpu().squeeze(0)\n",
    "            last_logits = logits[-1, :]\n",
    "\n",
    "            out_tok = int(sample(last_logits, temperature))\n",
    "            seed_seq.append(out_tok)\n",
    "            \n",
    "    return seed_seq\n",
    "\n",
    "def delete_by_indices(lst, indices):\n",
    "    indices_as_set = set(indices)\n",
    "    return [ lst[i] for i in range(len(lst)) if i not in indices_as_set ]\n",
    "\n",
    "def eval_ndfa(samples, w2i, i2w):\n",
    "  words = (['abc!', 'uvw!', 'klm!'])\n",
    "  abc = [w2i[x] for x in words[0]]\n",
    "  uvw =  [w2i[x] for x in words[1]]\n",
    "  klm =  [w2i[x] for x in words[2]]\n",
    "  words = [abc, uvw, klm]\n",
    " \n",
    "  correct = 0\n",
    "  for sample in samples:\n",
    "    \n",
    "    # sample =sample[0].tolist()\n",
    "    \n",
    "    # Delete .start and .end\n",
    "\n",
    "    sample = sample[1:-1]\n",
    "    # sample.pop(0)\n",
    "    print(\" \".join(decode(sample, i2w)))\n",
    "    \n",
    "    # sample.pop(-1)\n",
    "\n",
    "    if sample[0] != w2i['s'] or sample[-1] != w2i['s']:\n",
    "      print('Not start / end with s')\n",
    "      continue\n",
    "    \n",
    "    if w2i['.unk'] in sample or w2i['.start'] in sample:\n",
    "      print('unk or start in middle')\n",
    "      continue\n",
    "    \n",
    "    # First and last element MUST BE s at this point, delete them:\n",
    "    \n",
    "    if(len(sample) < 2): continue\n",
    "    sample.pop(0)\n",
    "    \n",
    "    sample.pop(-1)\n",
    "\n",
    "    if(len(sample) == 0): \n",
    "      correct += 1\n",
    "      continue\n",
    "\n",
    "    if w2i['s'] in sample:\n",
    "      print('rogue s spotted')\n",
    "      continue\n",
    "    \n",
    "    if len(sample) % 4 != 0:\n",
    "      print('words not % 4')\n",
    "      continue\n",
    "\n",
    "    if sample[0:4] == words[0]:\n",
    "      while len(sample) >= 4:\n",
    "        sample = delete_by_indices(sample, [0,1,2,3])\n",
    "      \n",
    "        if len(sample) == 0: \n",
    "          correct += 1\n",
    "          continue\n",
    "\n",
    "        if sample[0:4] != words[0]:\n",
    "          print('different word 0 or sth')\n",
    "          continue \n",
    "\n",
    "    if sample[0:4] == words[1]:\n",
    "      while len(sample) >= 4:\n",
    "        sample = delete_by_indices(sample, [0,1,2,3])\n",
    "        if len(sample) == 0:\n",
    "           correct += 1\n",
    "           continue\n",
    "\n",
    "        if sample[0:4] != words[1]:\n",
    "          print('different word 1 or sth')\n",
    "          continue \n",
    "\n",
    "    if sample[0:4] == words[2]:\n",
    "      while len(sample) >= 4:\n",
    "        sample = delete_by_indices(sample, [0,1,2,3])\n",
    "       \n",
    "        if len(sample) == 0:\n",
    "           correct += 1\n",
    "           continue\n",
    "\n",
    "        if sample[0:4] != words[2]:\n",
    "          print('different word or sth')\n",
    "          continue \n",
    "\n",
    "   \n",
    "\n",
    "  accuracy = correct / len(samples[0])\n",
    "  \n",
    "  return accuracy\n",
    "\n",
    "def main(dataset: str, max_chars_per_batch: int, net_hparams:HParams, n_samples: int, max_len: int, temperature: float, device, loss_fig='q456.png'):\n",
    "    logging.info(f'Loading dataset: {dataset}')\n",
    "    dataset_loader = dataset_loaders[dataset]\n",
    "    x_train, (i2w, w2i) = dataset_loader(n=150_000)\n",
    "    \n",
    "    logging.info(f'Creating batches of max chars: {max_chars_per_batch}')\n",
    "    batches = prepare_batches(x_train, i2w, w2i, max_chars_per_batch)\n",
    "\n",
    "\n",
    "    logging.info(f'Training on: {device}')\n",
    "\n",
    "    model = recurNet(\n",
    "        embedding_dim=net_hparams.EMB_DIM, \n",
    "        num_layers=net_hparams.N_LAYERS,\n",
    "        hidden_size=net_hparams.N_HIDDEN, \n",
    "        vocab_size=len(i2w)\n",
    "    )\n",
    "    \n",
    "    data, model = train(\n",
    "        model=model,\n",
    "        epochs=net_hparams.EPOCHS, \n",
    "        batches=batches, \n",
    "        device=device,\n",
    "        lr=net_hparams.LEARNING_RATE\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    logging.info(f'saving progress to: {loss_fig}')\n",
    "    df.groupby(by='epoch').mean()['loss'].plot()\n",
    "    plt.savefig(loss_fig)\n",
    "\n",
    "    return model, w2i, i2w\n",
    "\n",
    "def generate_samples(seq, model, n_samples, device, max_len, temperature):\n",
    "\n",
    "    logging.debug(f'Sampling from model with init seed: {\", \".join(decode(seq, i2w))}')\n",
    "\n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        out_seq = sample_model(model=model, seed_seq=seq, end_token= w2i['.end'], device=device, max_len=max_len, temperature=temperature)\n",
    "        out_seq_str = decode(out_seq, i2w)\n",
    "        logging.debug(f'Output-{i}: {\", \".join(out_seq_str)} [len={len(out_seq)}]')\n",
    "        samples.append(out_seq)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "dataset_loader = dataset_loaders[DATASET]\n",
    "x_train, (i2w, w2i) = dataset_loader(n=150_000)\n",
    "\n",
    "logging.info(f'Creating batches of max chars: {MAX_CHARS_PER_BATCH}')\n",
    "batches = prepare_batches(x_train, i2w, w2i, MAX_CHARS_PER_BATCH)\n",
    "\n",
    "# %%\n",
    "print_some_batches(batches, i2w)\n",
    "# %%\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "DATASET = 'ndfa'\n",
    "model, w2i, i2w = main(\n",
    "    dataset=DATASET,\n",
    "    max_chars_per_batch=MAX_CHARS_PER_BATCH,\n",
    "    net_hparams=HParams,\n",
    "    n_samples=EVAL_N_SAMPLES,\n",
    "    max_len=EVAL_SEQ_MAX_LEN,\n",
    "    temperature=TEMPERATURE,\n",
    "    device=device\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_ndfa_on_test_sequences():\n",
    "    test_sequences = [\n",
    "        [w2i['.start'], w2i['s'], w2i['k'], w2i['l']],\n",
    "        [w2i[c] for c in '.start s a b c ! a '.split()]\n",
    "    ]\n",
    "    for seq in test_sequences:\n",
    "        print(f'input | {\" \".join(decode(seq,i2w))}')\n",
    "        samples = generate_samples(seq, model, 10, device, EVAL_SEQ_MAX_LEN, TEMPERATURE)\n",
    "        acc = eval_ndfa(samples, w2i, i2w)\n",
    "        print(f'Accuracy: {acc}')\n",
    "\n",
    "eval_ndfa_on_test_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "DATASET = 'brackets'\n",
    "class HParams:\n",
    "    EMB_DIM = 32\n",
    "    N_HIDDEN = 16\n",
    "    N_LAYERS = 1\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.01\n",
    "\n",
    "model, w2i, i2w = main(\n",
    "    dataset=DATASET,\n",
    "    max_chars_per_batch=MAX_CHARS_PER_BATCH,\n",
    "    net_hparams=HParams,\n",
    "    n_samples=EVAL_N_SAMPLES,\n",
    "    max_len=EVAL_SEQ_MAX_LEN,\n",
    "    temperature=TEMPERATURE,\n",
    "    device=device,\n",
    "    loss_fig='brackets.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_brackets_on_test_sequences(model, device):\n",
    "    test_sequences = [\n",
    "        [w2i['.start'], w2i['('], w2i['('], w2i[')']],\n",
    "    ]\n",
    "    for seq in test_sequences:\n",
    "        print(f'input | {\" \".join(decode(seq,i2w))}')\n",
    "        samples = generate_samples(seq, model, 10, device, EVAL_SEQ_MAX_LEN, TEMPERATURE)\n",
    "        for s in samples:\n",
    "            print(' '.join(decode(s,i2w)))\n",
    "        # acc = eval_ndfa(samples, w2i, i2w)\n",
    "        # print(f'Accuracy: {acc}')\n",
    "eval_brackets_on_test_sequences(model, device)\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
